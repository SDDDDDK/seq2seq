{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "![Embedding](./img/embedding.png)\n",
    "\n",
    "onehotベクトルを入力とする全結合層。onehotベクトルの$1$となるインデックス$\\rm idx$を入力として、そこに相当する行を抜き出す。\n",
    "\n",
    "$$\n",
    "{\\bf h} = {\\bf W}_{{\\rm idx}, *}\n",
    "$$\n",
    "\n",
    "### サイズ\n",
    "|　変数　|　名前　|　サイズ　|\n",
    "|:---:|:---:|:---:|\n",
    "|-|onehotベクトル|`(n_alphabet)`|\n",
    "|${\\rm idx}$|インデックス|`(1)`|\n",
    "|${\\bf h}$|埋め込みベクトル|`(embed_dim)`|\n",
    "|${\\bf W}$|重み|`(n_alphabet, embed_dim)`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 4, 5],\n",
      "        [4, 0, 2, 9]])\n",
      "tensor([[[ 1.6995,  0.9900, -0.4197],\n",
      "         [-0.6903, -1.0622,  0.0646],\n",
      "         [-0.7134,  0.3108,  0.2643],\n",
      "         [-0.6781, -0.6527,  0.7753]],\n",
      "\n",
      "        [[-0.7134,  0.3108,  0.2643],\n",
      "         [ 1.6995,  0.9900, -0.4197],\n",
      "         [-0.6903, -1.0622,  0.0646],\n",
      "         [ 0.7821,  0.2614, -3.2782]]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0819,  0.0736,  0.9592],\n",
      "         [-0.2574,  0.0104,  1.1147],\n",
      "         [-0.8644,  0.2818,  1.0298]],\n",
      "\n",
      "        [[-0.2574,  0.0104,  1.1147],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0819,  0.0736,  0.9592],\n",
      "         [-0.2122,  1.0935, -0.4656]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "n_alphabet = 10; embed_dim = 3\n",
    "embedding = nn.Embedding(n_alphabet, embed_dim)\n",
    "idxs = torch.LongTensor([[0,2,4,5],[4,0,2,9]])\n",
    "print(idxs)\n",
    "embed = embedding(idxs)\n",
    "print(embed)\n",
    "\n",
    "# zero_padding\n",
    "PAD = 0\n",
    "embedding = nn.Embedding(n_alphabet, embed_dim, padding_idx=PAD)\n",
    "embed = embedding(idxs)\n",
    "print(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "![LSTM](./img/lstm.png)\n",
    "\n",
    "入力${\\bf x}_t$ `(in_dim)`と前時刻の隠れ状態${\\bf h}_{t-1}$ `(hid_dim)`を使用して、記憶セル${\\bf c}_{t-1}$ `(hid_dim)`と隠れ状態${\\bf h}_{t}$ `(hid_dim)`を更新する。\n",
    "\n",
    "$$\n",
    "{\\bf c}_t = {\\bf f}_t{\\bf c}_{t-1} + {\\bf i}_t{\\bf g}_t\\\\\n",
    "{\\bf h}_t = {\\bf o}_t \\tanh\\left({\\bf c}_t\\right)\n",
    "$$\n",
    "\n",
    "前時刻の記憶セル${\\bf c}_{t-1}$をどれだけ保持するか決める忘却ゲート${\\bf f}_t$：\n",
    "$$\n",
    "{\\bf f}_t = \\sigma\\left( {\\bf W}_{if}{\\bf x}_t + {\\bf W}_{hf}{\\bf h}_{t-1} \\right)\n",
    "$$\n",
    "\n",
    "記憶セルに加算する${\\bf g}_t$とそれの反映率を決める入力ゲート${\\bf i}_t$：\n",
    "$$\n",
    "{\\bf i}_t = \\sigma\\left( {\\bf W}_{ii}{\\bf x}_t + {\\bf W}_{hi}{\\bf h}_{t-1} \\right)\\\\\n",
    "{\\bf g}_t = \\tanh\\left( {\\bf W}_{ig}{\\bf x}_t + {\\bf W}_{hg}{\\bf h}_{t-1} \\right)\\\\\n",
    "$$\n",
    "\n",
    "隠れ状態${\\bf h}_{t}$にどれだけ記憶セルの内容$\\tanh( {\\bf c}_t )$を反映するかを決める出力ゲート${\\bf o}_t $：\n",
    "$$\n",
    "{\\bf o}_t = \\sigma\\left( {\\bf W}_{io}{\\bf x}_t + {\\bf W}_{ho}{\\bf h}_{t-1} \\right)\n",
    "$$\n",
    "\n",
    "### サイズ\n",
    "|　変数　|　名前　|　サイズ　|\n",
    "|:---:|:---:|:---:|\n",
    "|${\\bf x}$|入力|`(in_dim)`|`(seq, batch_size, in_dim)`|\n",
    "|${\\bf h}$|隠れ状態|`(hid_dim)`|\n",
    "|${\\bf c}$|記憶セル|`(hid_dim)`|\n",
    "|${\\bf W}_{i*}$|入力にかける重み|`(hid_dim, in_dim)`|\n",
    "|${\\bf W}_{h*}$|隠れ状態にかける重み|`(hid_dim, hid_dim)`|\n",
    "|${\\bf i, g, f, o}$|各ゲート等|`(hid_dim)`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3707,  1.6203,  0.9399]],\n",
      "\n",
      "        [[ 1.1986, -0.2543, -0.2785]],\n",
      "\n",
      "        [[-0.1050, -2.2704, -1.5767]],\n",
      "\n",
      "        [[ 0.4714,  0.3945,  1.2863]],\n",
      "\n",
      "        [[-1.3864,  0.2574, -0.0920]]])\n",
      "tensor([[[-0.4008, -0.0932,  0.2109]],\n",
      "\n",
      "        [[-0.3955,  0.1253,  0.1145]],\n",
      "\n",
      "        [[-0.1316,  0.1825, -0.0205]],\n",
      "\n",
      "        [[-0.5473,  0.1051,  0.1221]],\n",
      "\n",
      "        [[-0.0443,  0.1136,  0.1150]]], grad_fn=<CatBackward>)\n",
      "(tensor([[[-0.0443,  0.1136,  0.1150]]], grad_fn=<ViewBackward>), tensor([[[-0.1209,  0.2656,  0.2066]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "in_dim = 3; hid_dim = 3; batch_size = 5;\n",
    "lstm = nn.LSTM(in_dim, hid_dim)\n",
    "inputs = [torch.randn(1, in_dim) for _ in range(batch_size)]\n",
    "inputs = torch.cat(inputs).view(len(inputs), -1, in_dim) # seq, batch, feature(default: batch_first=False)\n",
    "print(inputs)\n",
    "\n",
    "# initialize the cell hidden state.\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)    # each hidden\n",
    "print(hidden) # (last cell, last hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
